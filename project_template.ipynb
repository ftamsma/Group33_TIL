{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Group - 33\n",
    "\n",
    "Members: Amber van der Helm, Femke Tamsma, Merel Loman, Naomi Rottier, and Robin Karthaus\n",
    "\n",
    "Student numbers: 5164303, 5122422, 4852982, 5496462, 5634563"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Objective\n",
    "\n",
    "*Requires data modeling and quantitative research in Transport, Infrastructure & Logistics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bin packing is an Operational Research (OR) technique. It is similar to the knapsack problem, however the knapsack problem is a maximization problem where the purpose is to fill a fixed-size knapsack with the most valuable items. Whereas in the bin packing problem all items should be assigned and the purpose is to minimize the number of used bins. \n",
    "\n",
    "Throughout the supply chain various applications can be found where a bin packing problem is encountered. From assigning cargo to airplanes or containers to assigning packages to trucks. \n",
    "\n",
    "The objective for this research is a model that is able to optimize the allocation of packages to vehicles. This is first done with one type of vehicle with the same characteristics. From this model the model will be extended so it can optimize the same problem with different types of vehicles that can be used and eventually the emissions of the vehicles will be considered as well. Even more applications can be added to extend the model to become more realistic. An example is not allocating all packages from one day but to differentiate in parts of the day and that packages from certain time slots are used, instead of all packages available.  \n",
    "\n",
    "The data set from the \"Amazon Last Mile Routing Challenge Dataset\" will be used. This data contains information about the dimension of the packages being delivered. The data will first be imported to Python and converted from a JSON file to a CSV file. Missing values will either be deleted or replaced by the average values, depending on the outcomes of the descriptive analysis. To simplify the model, the three dimensions (length, width, height), will be converted to a new variable (column) \"volume\" in cm3. Since the dimension of the vans is also in cm3, optimizing the bin packing algorithm will be possible. After these first data processing steps, the data will be further cleaned by checking for outliers and deleting these if necessary. Also a general view of the data will be formed by descriptive statistics and corresponding visualizations (box plots, normal distributions, histograms, scatter plots, etc.). \n",
    "\n",
    "When all the data is cleaned and a general view of the data is formed the algorithm will be written. This algorithm will be a bin packing algorithm which optimizes the allocation/loading of the vans. This algorithm will be written in different steps. First many assumptions will be made to keep the algorithm as simple as possible (for example, only one van at a time can be loaded). When the model runs correctly, the algorithm will be made more complex by adding more constraints and making less assumptions. Each \"complexity step\" in the model will be monitored separately and give insight into the capacity of the model of optimizing the loading more optimally. A visualization regarding the decrease in total needed vans per increase in model complexity will be created. \n",
    "\n",
    "The research question used in this assignment is: \"Is it possible to optimize the allocation of packages, given by the Amazon dataset, to vans using a binpacking algorithm, where model complexity steps are taken into account and visualization of these complexity steps are done with advanced visualisation?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribution Statement\n",
    "\n",
    "*Be specific. Some of the tasks can be coding (expect everyone to do this), background research, conceptualisation, visualisation, data analysis, data modelling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author 1** (Amber): converting data to CSV\n",
    "\n",
    "**Author 2** (Femke): visualization possibilities \n",
    "\n",
    "**Author 3** (Merel): converting data to CSV\n",
    "\n",
    "**Author 4** (Naomi): checking other projects and RQs\n",
    "\n",
    "**Author 5** (Robin): visualization possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used 2 datafiles from Amazon to get the dimensions of the packages and delivery vans. The documentation can be found here: https://github.com/MIT-CAVE/rc-cli/blob/main/templates/data_structures.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "json_file_path_package = 'package_data.json'\n",
    "\n",
    "with open(json_file_path_package, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "json_file_path_route = 'route_data.json'\n",
    "\n",
    "with open(json_file_path_route, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series of steps to clean the raw data, after which we process and store it in a way that we can use it for our research question. First, the json data of the packages will be pipelined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "json_file_path = 'package_data.json'\n",
    "\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize empty lists for each column\n",
    "package_ids = []\n",
    "scan_statuses = []\n",
    "start_times = []\n",
    "end_times = []\n",
    "service_times = []\n",
    "depths = []\n",
    "heights = []\n",
    "widths = []\n",
    "stop_ids = []\n",
    "route_ids = []\n",
    "\n",
    "# Iterate through the JSON data to extract information\n",
    "for route_id, route_data in data.items():\n",
    "    for stop_id, stop_data in route_data.items():\n",
    "        for package_id, package_data in stop_data.items():\n",
    "            package_ids.append(package_id)\n",
    "            scan_statuses.append(package_data.get(\"scan_status\", \"\"))\n",
    "            time_window = package_data.get(\"time_window\", {})\n",
    "            start_times.append(time_window.get(\"start_time_utc\", \"\"))\n",
    "            end_times.append(time_window.get(\"end_time_utc\", \"\"))\n",
    "            service_times.append(package_data.get(\"planned_service_time_seconds\", \"\"))\n",
    "            dimensions = package_data.get(\"dimensions\", {})\n",
    "            depths.append(dimensions.get(\"depth_cm\", \"\"))\n",
    "            heights.append(dimensions.get(\"height_cm\", \"\"))\n",
    "            widths.append(dimensions.get(\"width_cm\", \"\"))\n",
    "            stop_ids.append(stop_id)\n",
    "            route_ids.append(route_id)\n",
    "\n",
    "# Create a pandas DataFrame from the extracted data\n",
    "df = pd.DataFrame({\n",
    "    \"PackageID\": package_ids,\n",
    "    \"ScanStatus\": scan_statuses,\n",
    "    \"StartTimeUTC\": start_times,\n",
    "    \"EndTimeUTC\": end_times,\n",
    "    \"PlannedServiceTimeSeconds\": service_times,\n",
    "    \"DepthCM\": depths,\n",
    "    \"HeightCM\": heights,\n",
    "    \"WidthCM\": widths,\n",
    "    \"StopID\": stop_ids,\n",
    "    \"RouteID\": route_ids\n",
    "})\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the raw data of the packages is filtered, the second json file will be cleaned in order to get useful route data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "json_file_path_route = 'route_data.json'\n",
    "\n",
    "with open(json_file_path_route, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "# Initialize empty lists for each column\n",
    "route_ids = []\n",
    "station_codes = []\n",
    "dates = []\n",
    "departure_times = []\n",
    "executor_capacities = []\n",
    "route_scores = []\n",
    "stop_ids = []\n",
    "lats = []\n",
    "lngs = []\n",
    "types = []\n",
    "zone_ids = []\n",
    "\n",
    "# Iterate through the JSON data to extract information\n",
    "for route_id, route_data in data.items():\n",
    "    station_code = route_data.get(\"station_code\", \"\")\n",
    "    date = route_data.get(\"date_YYYY_MM_DD\", \"\")\n",
    "    departure_time_utc = route_data.get(\"departure_time_utc\", \"\")\n",
    "    executor_capacity_cm3 = route_data.get(\"executor_capacity_cm3\", \"\")\n",
    "    route_score = route_data.get(\"route_score\", \"\")\n",
    "    \n",
    "    stops = route_data.get(\"stops\", {})\n",
    "    for stop_id, stop_data in stops.items():\n",
    "        lat = stop_data.get(\"lat\", \"\")\n",
    "        lng = stop_data.get(\"lng\", \"\")\n",
    "        stop_type = stop_data.get(\"type\", \"\")\n",
    "        zone_id = stop_data.get(\"zone_id\", \"\")\n",
    "        \n",
    "        route_ids.append(route_id)\n",
    "        station_codes.append(station_code)\n",
    "        dates.append(date)\n",
    "        departure_times.append(departure_time_utc)\n",
    "        executor_capacities.append(executor_capacity_cm3)\n",
    "        route_scores.append(route_score)\n",
    "        stop_ids.append(stop_id)\n",
    "        lats.append(lat)\n",
    "        lngs.append(lng)\n",
    "        types.append(stop_type)\n",
    "        zone_ids.append(zone_id)\n",
    "\n",
    "# Create a pandas DataFrame from the extracted data\n",
    "df = pd.DataFrame({\n",
    "    \"RouteID\": route_ids,\n",
    "    \"station_code\": station_codes,\n",
    "    \"date_YYYY_MM_DD\": dates,\n",
    "    \"departure_time_utc\": departure_times,\n",
    "    \"executor_capacity_cm3\": executor_capacities,\n",
    "    \"route_score\": route_scores,\n",
    "    \"stop_id\": stop_ids,\n",
    "    \"lat\": lats,\n",
    "    \"lng\": lngs,\n",
    "    \"type\": types,\n",
    "    \"zone_id\": zone_ids\n",
    "})\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
